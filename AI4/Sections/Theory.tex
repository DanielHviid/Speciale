\section{Theory}

In this section we look at the teory behind Spiking neural networks, reservoir computing, and Liquid State Machines, assuming that the reader is familiar with Artificial neural networks.

\subsection{Spiking Neural Network}

The main difference between a Spiking Neural Network and a non-Spiking Neural Network is the way it processes the weighted input sum in order to find the output. For a non-Spiking NN, this is the activation function, while a Spiking NN uses a model that stores information from activation to activation. This main difference is what provides the Spiking NN with temporal information that permeates through the network.

Several such models exist, but only the Leaky Integrate-and-fire model \cite{Leaky} will be explained and used in this project.

The Leaky Integrate-and-fire model attempts to emulate the membrane potential and voltage spikes of a biological neuron. The biological neuron uses two different ion channels in order to build up potential and then dump it as a spike, which is modelled using the following equation, from \cite{leaky}:

\begin{equation}
\tau_m\frac{dv}{dt} = -v(t) + RI(t)
\end{equation}

Where [$\tau_m$] is the time constant, [$v(t)$] is the membrane potential at a given time t, R is the resistance of the membrane, and $I(t)$ is the integral of the input current.
There are three different parts to this. The first is the change in membrane potential; This change in potential is the main part of the equation, as it is what we wish to find at each time step. The second part is the ``leaky'' part of the Leaky Integrate-and-fire, where [$v(t)$] is the membrane potential at a given time t. At each timestep a part of the stored potential is lost, which is proportional in size with the stored potential. The last part is the integral of the weighted inputs, where R is the resistance of the membrane, and $I(t)$ is the integral of the input current, as with a non-Spiking NN. The time constant [$\tau_m$] then scales all of this to fit with the time between each activation.

\subsection{Reservoir computing}

A reservoir Neural Network is an extension to an ANN where the hidden layer is made in such a way that instead of having several layers that feed to each other in order, the individual neurons are simply randomly connected to each other with random weights.