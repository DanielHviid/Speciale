\section{Discussion}

In this section we discuss the Liquid State Machine, more specifically the choises taken when implementing it, the troubles of tuning it, and the results of the tests.

When implementing a neural network, one would usually make a set of matrices and tensors that contain the inputs, outputs, activations and weights. In this implementation, however, a graph implementation with nodes for neurons was used. This impacted the computational time of the network, but also allowed for easier bugfixing and faster implementation. This tradeoff was made because of the limited scope of the project, as well as the relatively small problem it was used on. This allowed for the implementation of the Neuron class, which made it much easier to implement it in a modular fashion.

Tuning a Liquid State Machine is a tedious process, as a small change in parameters will make the network lose a lot of accuracy. In particular the number of connections, neuron resistance, resting potential, proportion of inhibatory neurons, and number of synapses per input neurons. These parameters should be tuned quite gently, as they will make the network either almost inactive or start a chain reaction.